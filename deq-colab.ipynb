{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "900de460-2ee8-4e7a-f4c8-1170ec3fdc22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import jax.lax as lax\n",
        "from jax import random, jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable, Any\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEQ idea & finding stationary points with root finder, maybe root finder demo on small example (but that's close to copying from last year so maybe smth different?)"
      ],
      "metadata": {
        "id": "lEX0Pf1IGH2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, z0, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, ls=False):\n",
        "    bsz, total_hsize = z0.shape\n",
        "    orig_shape = (bsz,total_hsize)\n",
        "    seq_len = 1\n",
        "    new_shape = (bsz,total_hsize,seq_len)\n",
        "    z0 = z0.reshape(*new_shape)\n",
        "    def g(_z):\n",
        "        # here it is safe to use x out of scope\n",
        "        return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(*new_shape)\n",
        "    dev = z0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    z_est = z0           # (bsz, 2d, L')\n",
        "    gz = g(z_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_zest, lowest_gz = 0, z_est, gz\n",
        "\n",
        "    while nstep < threshold:\n",
        "        z_est, gz, delta_z, delta_gz, ite = line_search_jax(update, z_est, gz, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gz)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gz + z_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_zest, lowest_gz = jnp.copy(z_est), jnp.copy(gz)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_z)\n",
        "        u = (delta_z - matvec_jax(part_Us, part_VTs, delta_gz)) / jnp.einsum('bij, bij -> b', vT, delta_gz)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    lowest_zest = lowest_zest.reshape(*orig_shape)\n",
        "    # print(\"broyden\",jnp.linalg.norm(z_est),jnp.linalg.norm(gz))\n",
        "\n",
        "    if result_dict:\n",
        "        return {\"result\": lowest_zest,\n",
        "                \"lowest\": lowest_dict[stop_mode],\n",
        "                \"nstep\": lowest_step_dict[stop_mode],\n",
        "                \"prot_break\": prot_break,\n",
        "                \"abs_trace\": trace_dict['abs'],\n",
        "                \"rel_trace\": trace_dict['rel'],\n",
        "                \"eps\": eps,\n",
        "                \"threshold\": threshold}\n",
        "    else:\n",
        "        return lowest_zest\n",
        "\n",
        "\n",
        "def newton_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    # note: f might ignore x0 (i.e. with backward pass)\n",
        "    orig_shape = z0.shape\n",
        "    def g(_z):\n",
        "      # this reshaping is to enable solving with Jacobian\n",
        "      return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(-1)\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    z = z0.reshape(-1)\n",
        "    gz = g(z)\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    nstep = 0\n",
        "\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      # solve system\n",
        "      jgz = jac_g(z)\n",
        "      # print(\"gz\",gz.shape,jnp.linalg.norm(gz))\n",
        "      # print(\"jgz\",jgz.shape,jnp.linalg.norm(jgz))\n",
        "      delta_z = jnp.linalg.solve(jgz,-gz)\n",
        "      # print(\"delta_z\",delta_z.shape,jnp.linalg.norm(delta_z))\n",
        "      z = z + delta_z\n",
        "      # need to compute gx here to decide whether to stop\n",
        "      gz = g(z)\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      nstep += 1\n",
        "\n",
        "    z = z.reshape(*orig_shape).astype(jnp.float32)\n",
        "\n",
        "    # assert False\n",
        "\n",
        "    return z\n",
        "\n",
        "def direct_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    nstep = 0\n",
        "    z_old = z0\n",
        "    z_new = f(z0,x0)\n",
        "    gz = z_new-z_old\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    min_gz_norm, min_z = gz_norm, z_new\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      z_old = z_new\n",
        "      z_new = f(z_old,x0)\n",
        "      gz = z_new-z_old\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      if gz_norm < min_gz_norm:\n",
        "        min_gz_norm, min_z = gz_norm, z_new\n",
        "      nstep += 1\n",
        "    # print(\"min_gz_norm\",min_gz_norm,\"nstep\",nstep)\n",
        "    return min_z\n"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3) \n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        # inject original input if resolution=0 \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        # residual\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EzcfTWz6agE_"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        \n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        conv_down = nn.Conv(features=self.in_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        relu_down = nn.relu\n",
        "\n",
        "        for n in range(num_samples-1):\n",
        "            down_block += [conv_down, group_down, relu_down]\n",
        "        conv_down = nn.Conv(features=self.out_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        down_block += [conv_down, group_down]\n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cringy_reshape(in_vec, shape_list):\n",
        "    start = 0\n",
        "    out_vec = []\n",
        "    if isinstance(in_vec, list):\n",
        "        raise ValueError\n",
        "    # in_vec = jnp.array(in_vec)\n",
        "    for size in shape_list:\n",
        "        my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "        end = start+my_elems\n",
        "        my_chunk = jnp.copy(in_vec[:, start:end])\n",
        "        start += my_elems\n",
        "        my_chunk = jnp.reshape(my_chunk, size)\n",
        "        out_vec.append(my_chunk)\n",
        "\n",
        "    return out_vec\n",
        "        "
      ],
      "metadata": {
        "id": "LMLCzlwHO3A0"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps image to initial latent representation\n",
        "    AKA the grey part in the diagram\n",
        "    \"\"\"\n",
        "\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    training: bool = True\n",
        "\n",
        "    def setup(self):\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A tool for using the \n",
        "    \"\"\"\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=4\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DAYfgxMOGUNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[8, 16])\n",
        "    expansion: int = 4\n",
        "    final_chansize: int = 200\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def make_cls_block(self, in_chan, out_chan):\n",
        "          downsample = False\n",
        "          if in_chan != out_chan * self.expansion:\n",
        "              downsample = True\n",
        "          return CLSBlock(in_chan, out_chan, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        combine_modules = []\n",
        "        for i  in range(len(self.channels)):\n",
        "            output_mod = self.make_cls_block(self.channels[i], self.output_channels[i])\n",
        "            combine_modules.append(output_mod)\n",
        "        self.combine_modules = combine_modules\n",
        "\n",
        "        self.final_layer_conv = nn.Conv(self.final_chansize, kernel_size=(1,1))\n",
        "        self.final_layer_bn = nn.BatchNorm()\n",
        "\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, y):\n",
        "        y_final = self.combine_modules[0](y[0])\n",
        "        for i in range(len(self.channels)-1):\n",
        "            y_final = self.combine_modules[i+1](y[i+1]) \n",
        "        y_final = self.final_layer_bn(self.final_layer_conv(y_final), use_running_average=True)\n",
        "        y_final = nn.relu(y_final)\n",
        "        y_final = nn.avg_pool(y_final, window_shape=y_final.shape[1:3])\n",
        "        y_final = jnp.reshape(y_final, (y_final.shape[0], -1))\n",
        "        y_final = self.classifier(y_final)\n",
        "        return y_final"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    image = np.expand_dims(image, -1)\n",
        "    # image = np.tile(image, (1,1,1,24))\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.MNIST(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.MNIST(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data[:10000], train_ds.targets[:10000])\n",
        "    test_images, test_labels = transform(test_ds.data[:1000], test_ds.targets[:1000])\n",
        "    print(f\"MUM TRAINING IMAGES:::{train_images.shape[0]}\")\n",
        "    print(f\"MUM TEST IMAGES:::{test_images.shape[0]}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "def rootfind(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 z: jnp.ndarray,\n",
        "                 x: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, z, x, threshold, eps=1e-3))\n",
        "\n",
        "# Its forward call (basically just calling it)\n",
        "def _rootfind_fwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray,\n",
        "                      x: jnp.ndarray):\n",
        "    z = rootfind(solver_fn, f_fn, threshold, eps, weights, z, x)\n",
        "    # print(\"fwd residual\",jnp.linalg.norm(f_fn(weights,z,x)-z)/jnp.linalg.norm(z))\n",
        "    return z, (weights, z, x)\n",
        "\n",
        "# Its backward call (its inputs)\n",
        "def _rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,  \n",
        "                      grad):\n",
        "    weights, z, x = res\n",
        "    (_, vjp_fun) = jax.vjp(f_fn, weights, z, x)\n",
        "    def z_fn(z,x): # gets transpose Jac w.r.t. weights and z using vjp_fun\n",
        "        (Jw_T, Jz_T, _) = vjp_fun(z)\n",
        "        return Jz_T + grad\n",
        "    #def gimme_jzt(z):\n",
        "     #   (Jw_T, Jz_T, _) = vjp_fun(z)\n",
        "      #  return Jz_T\n",
        "\n",
        "    #z0 = jnp.zeros_like(grad)\n",
        "    key, subkey = random.split(jax.random.PRNGKey(0))\n",
        "    z0 = random.normal(subkey, grad.shape)\n",
        "    x0 = None # dummy, z_fn does not use x\n",
        "    g = solver_fn(z_fn, z0, x0, threshold, eps)\n",
        "    #Jz_T = gimme_jzt(z0)\n",
        "    #g = -jnp.transpose(jnp.linalg.pinv(Jz_T) * jnp.transpose(grad))\n",
        "    #print('diff broyden vs inv', jnp.linalg.norm(g - g_p))\n",
        "    #print(\"bwd residual\",jnp.linalg.norm(z_fn(g,x0)-g)/jnp.linalg.norm(g))\n",
        "    return (None, g, None)\n",
        "\n",
        "rootfind.defvjp(_rootfind_fwd, _rootfind_bwd)"
      ],
      "metadata": {
        "id": "q61ohtb5HKwj"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQFF(nn.Module):\n",
        "    \"\"\"\n",
        "    The f_{\\theta}(z,x) function that is repeatedly applied\n",
        "    AKA the yellow block in the diagram\n",
        "    \"\"\"\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "   \n",
        "    def setup(self):\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    array.append(sampled)\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  use_bias=False),\n",
        "                                           nn.relu,\n",
        "                                           nn.GroupNorm(num_groups=self.num_groups)]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, z, x, shape_tuple):\n",
        "        \n",
        "        batch_size = z.shape[0]\n",
        "        z_list = cringy_reshape(z,shape_tuple)\n",
        "        x_list = cringy_reshape(x,shape_tuple)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](z_list[i], i, x_list[i])) # z, branch, x\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                  intermediate_i += branch_outputs[j]\n",
        "                else:\n",
        "                    if self.fuse_branches[i][j] is not None:\n",
        "                        temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                        intermediate_i += temp\n",
        "                    else:\n",
        "                        raise Exception(\"Should not happen.\")\n",
        "            fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "          # stick z back into into one vector\n",
        "        fuse_outputs = jnp.concatenate([fo.reshape(batch_size,-1) for fo in fuse_outputs],axis=1)\n",
        "        assert fuse_outputs.shape[1] == z.shape[1]\n",
        "        return fuse_outputs\n"
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mdeq_inputs(x,num_branches):\n",
        "\n",
        "  batch_size = x.shape[0]\n",
        "  x_list = [x]\n",
        "  for i in range(1, num_branches):\n",
        "      bs, H, W, y = x_list[-1].shape\n",
        "      new_item = jnp.zeros((bs, H//2, W//2, y))\n",
        "      x_list.append(new_item)\n",
        "  z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "  shape_list = [el.shape for el in z_list]\n",
        "  # make them (batched) vectors\n",
        "  x_vec = jnp.concatenate([x.reshape(batch_size,-1) for x in x_list],axis=1)\n",
        "  z_vec = jnp.concatenate([z.reshape(batch_size,-1) for z in z_list],axis=1)\n",
        "  # i'm not sure if tuple is actually important but I like it for non-mutability\n",
        "  shape_tuple = tuple(shape_list)\n",
        "  return x_vec, z_vec, shape_tuple\n",
        "\n",
        "\n",
        "def mdeq_fn(x,encoder,decoder,deqff,all_weights,solver_fn=None,mode='broyden'):\n",
        "    threshold = 7\n",
        "    eps = 1e-3\n",
        "    encoder_weights = all_weights[\"encoder\"]\n",
        "    decoder_weights = all_weights[\"decoder\"]\n",
        "    deqff_weights = all_weights[\"mdeqff\"]\n",
        "    batch_size = x.shape[0]\n",
        "    # transform the input image\n",
        "    x = encoder.apply(encoder_weights,x)\n",
        "    # construct inputs (lots of padding and concatenation)\n",
        "    x, z, shape_tuple = create_mdeq_inputs(x,deqff.num_branches)\n",
        "    if mode == 'broyden':\n",
        "      # the root function can only take 3 ndarrays as input\n",
        "      def deqff_root(_weights,_z,_x):\n",
        "        # note: it's safe to pass the shape_tuple here (no tracers)\n",
        "        return deqff.apply(_weights,_z,_x,shape_tuple)\n",
        "      # apply rootfinder with custom vjp\n",
        "      z = rootfind(solver_fn,deqff_root,threshold,eps,deqff_weights,z,x)\n",
        "  \n",
        "    elif mode == \"direct_solver\":\n",
        "        max_evals = 5\n",
        "        threshold=1e-2\n",
        "        evals = 0\n",
        "        residual = jnp.inf\n",
        "        while evals < max_evals and residual > threshold:\n",
        "          z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "          f_z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "          residual = jnp.linalg.norm(f_z - z) / jnp.linalg.norm(z)\n",
        "          evals += 1\n",
        "      \n",
        "    elif mode in [\"predict\", \"warmup\"]:\n",
        "      z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "\n",
        "    z_list = cringy_reshape(z,shape_tuple)\n",
        "    log_probs = decoder.apply(decoder_weights,z_list)\n",
        "\n",
        "    f_z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "    residuals = jnp.linalg.norm(f_z - z) / jnp.linalg.norm(z)\n",
        "    # reshape back to list\n",
        "    \n",
        "    return log_probs, residuals"
      ],
      "metadata": {
        "id": "5hI7s8DE1kqX"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    ''' \n",
        "    should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "    if getting funny results maybe remove log of logits\n",
        "    '''\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
        "    output = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "    return output, acc"
      ],
      "metadata": {
        "id": "r8M7Sf_gmqD7"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(images,labels,encoder,decoder,deqff,all_weights):\n",
        "    loss, acc = 0,0\n",
        "    batch_size = 128\n",
        "    start, end = 0, 0\n",
        "    loss_vals = []\n",
        "    acc_vals = []\n",
        "    while end < images.shape[0]:\n",
        "        end = min(start+batch_size, images.shape[0])\n",
        "        x_batch = images[start:end]\n",
        "        x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "\n",
        "        y_true = labels[start:end]\n",
        "        start = end\n",
        "\n",
        "        log_probs, residual = mdeq_fn(x_batch,encoder,decoder,deqff,all_weights,mode='predict')\n",
        "        loss, acc = cross_entropy_loss(log_probs, y_true)\n",
        "        loss_vals.append(loss * x_batch.shape[0])\n",
        "        acc_vals.append(acc * x_batch.shape[0])\n",
        "    return sum(jnp.array(loss_vals)) / images.shape[0], sum(jnp.array(acc_vals)) / images.shape[0], residual"
      ],
      "metadata": {
        "id": "1kUlVk4z0LOV"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(mode=\"broyden\"):\n",
        "\n",
        "    assert mode in [\"broyden\", \"direct_solver\", \"warmup\"], \"INCORRECT MODE\"\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    num_images = train_images.shape[0]\n",
        "    image_size = train_images.shape[1]\n",
        "    batch_size = 128\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    solver_fn = direct_jax\n",
        "\n",
        "    num_groups = 8\n",
        "    channels = [24, 24]\n",
        "    num_branches = 2\n",
        "\n",
        "    # instantiation\n",
        "    encoder = Encoder(channels=channels)\n",
        "    decoder = Classifier() # not sure about what to pass\n",
        "    mdeqff = MDEQFF(num_branches=num_branches, channels=channels, num_groups=num_groups)\n",
        "\n",
        "    # weight initialization\n",
        "    prng = jax.random.PRNGKey(0)\n",
        "    prng, _ = jax.random.split(prng, 2)\n",
        "    x_dummy = jnp.ones((batch_size, image_size, image_size, 24))\n",
        "    x_dummy_2, encoder_weights = encoder.init_with_output(prng,x_dummy)\n",
        "    x_dummy_3, z_dummy, shape_tuple = create_mdeq_inputs(x_dummy_2,num_branches)\n",
        "    z_dummy_2, mdeqff_weights = mdeqff.init_with_output(prng,z_dummy,x_dummy_3,shape_tuple)\n",
        "    z_dummy_3 = cringy_reshape(z_dummy_2,shape_tuple)\n",
        "    o_dummy, classifier_weights = decoder.init_with_output(prng,z_dummy_3)\n",
        "\n",
        "    # collect weights\n",
        "    weights = {'encoder': encoder_weights, 'mdeqff': mdeqff_weights ,'decoder': classifier_weights}\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "\n",
        "    def loss(weights, x_batch, y_true, mode):\n",
        "        logits, residual = mdeq_fn(x_batch,encoder,decoder,mdeqff,weights,solver_fn,mode)\n",
        "        loss, acc = loss_fn(logits, y_true)\n",
        "        return loss, (acc, residual)\n",
        "\n",
        "    def step(weights, opt_state, x_batch, y_true, mode):\n",
        "        (loss_vals, (acc, residual)), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true, mode)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc, residual\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=num_images, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler(seed):\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "        return shuffled_indices\n",
        "    \n",
        "    max_epoch = 50\n",
        "    warmup_epochs = 0\n",
        "    if mode == \"warmup\":\n",
        "      warmup_epochs = 2\n",
        "      max_epoch += warmup_epochs\n",
        "    print_interval = 1\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    if mode == \"warmup\":\n",
        "        print(f\"TRAINING MODE:::{mode}+broyden\")\n",
        "    else:\n",
        "        print(f\"TRAINING MODE:::{mode}\")\n",
        "    train_loss_vals, val_loss_vals = [], []\n",
        "    train_acc_vals, val_acc_vals = [], []\n",
        "    train_res_vals, val_res_vals = [], []\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "        if mode == \"warmup\":\n",
        "          if epoch >= warmup_epochs:\n",
        "            mode = \"broyden\"\n",
        "            print(\"-------------DONE WARM UP---------------\")\n",
        "          elif epoch == 0:\n",
        "            print(\"----------STARTING WARM UP--------------\")\n",
        "            \n",
        "        idxs = list_shuffler(epoch)\n",
        "        start, end = 0, 0\n",
        "\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        res_vals = []\n",
        "\n",
        "        counter = 0\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab,...]\n",
        "            x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "  \n",
        "            weights, opt_state, batch_loss, batch_acc, batch_res = step(weights=weights,\n",
        "                                                                        opt_state=opt_state,\n",
        "                                                                        x_batch=x_batch,\n",
        "                                                                        y_true=y_true,\n",
        "                                                                        mode=mode)\n",
        "            \n",
        "            counter += 1\n",
        "\n",
        "            loss_vals.append(batch_loss * x_batch.shape[0])\n",
        "            acc_vals.append(batch_acc * x_batch.shape[0])\n",
        "            res_vals.append(batch_res * x_batch.shape[0])\n",
        "\n",
        "\n",
        "            print(f\"batch_loss {counter} :: {batch_loss} // batch_acc :: {batch_acc} // batch_res :: {batch_res} \")\n",
        "\n",
        "        epoch_loss = sum(jnp.array(loss_vals)) / len(idxs)\n",
        "        epoch_acc = sum(jnp.array(acc_vals)) / len(idxs)\n",
        "        epoch_res = sum(jnp.array(res_vals)) / len(idxs)\n",
        "        \n",
        "        train_loss_vals.append(epoch_loss)\n",
        "        train_acc_vals.append(epoch_acc)\n",
        "        train_res_vals.append(epoch_res)\n",
        "\n",
        "        val_loss, val_acc, val_res = predict(test_images,test_labels,encoder,decoder,mdeqff,weights)\n",
        "\n",
        "        val_loss_vals.append(val_loss)\n",
        "        val_acc_vals.append(val_acc)\n",
        "        val_res_vals.append(val_res)\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(f\"\\tTRAIN epoch = {epoch} / loss = {epoch_loss} / acc = {epoch_acc} / res = {epoch_res}\")\n",
        "            print(f\"\\tVAL epoch = {epoch} / loss = {val_loss} / acc = {val_acc} / res = {val_res}\")\n",
        "\n",
        "        if epoch_loss < 1e-5:\n",
        "            break\n",
        "\n",
        "            print('finally', batch_loss)\n",
        "    results = {'train_loss_vals': train_loss_vals,\n",
        "               'train_acc_vals': train_acc_vals,\n",
        "               'train_res_vals': train_res_vals,\n",
        "               'val_loss_vals': val_loss_vals,\n",
        "               'val_acc_vals': val_acc_vals,\n",
        "               'val_res_vals': val_res_vals}\n",
        "    return results"
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broyden_results = train(mode=\"direct_solver\")"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db372000-8e54-4144-ba25-9e4845998e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUM TRAINING IMAGES:::10000\n",
            "MUM TEST IMAGES:::1000\n",
            "TRAINING MODE:::direct_solver\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_loss 1 :: 2.441256523132324 // batch_acc :: 0.046875 // batch_res :: 0.9842714667320251 \n",
            "batch_loss 2 :: 2.3334712982177734 // batch_acc :: 0.109375 // batch_res :: 1.1276499032974243 \n",
            "batch_loss 3 :: 2.3200523853302 // batch_acc :: 0.078125 // batch_res :: 1.114075779914856 \n",
            "batch_loss 4 :: 2.2909717559814453 // batch_acc :: 0.0859375 // batch_res :: 1.1303197145462036 \n",
            "batch_loss 5 :: 2.3241219520568848 // batch_acc :: 0.125 // batch_res :: 1.1538829803466797 \n",
            "batch_loss 6 :: 2.300154209136963 // batch_acc :: 0.1171875 // batch_res :: 1.158766269683838 \n",
            "batch_loss 7 :: 2.3144173622131348 // batch_acc :: 0.140625 // batch_res :: 1.1834536790847778 \n",
            "batch_loss 8 :: 2.3102312088012695 // batch_acc :: 0.078125 // batch_res :: 1.142022967338562 \n",
            "batch_loss 9 :: 2.291822910308838 // batch_acc :: 0.15625 // batch_res :: 1.0864394903182983 \n",
            "batch_loss 10 :: 2.299612045288086 // batch_acc :: 0.09375 // batch_res :: 1.061159372329712 \n",
            "batch_loss 11 :: 2.3034019470214844 // batch_acc :: 0.09375 // batch_res :: 1.0354280471801758 \n",
            "batch_loss 12 :: 2.2914066314697266 // batch_acc :: 0.125 // batch_res :: 1.0213170051574707 \n",
            "batch_loss 13 :: 2.2937564849853516 // batch_acc :: 0.0859375 // batch_res :: 1.021909475326538 \n",
            "batch_loss 14 :: 2.2768354415893555 // batch_acc :: 0.1015625 // batch_res :: 1.01266610622406 \n",
            "batch_loss 15 :: 2.2515649795532227 // batch_acc :: 0.2421875 // batch_res :: 0.990810751914978 \n",
            "batch_loss 16 :: 2.2497174739837646 // batch_acc :: 0.234375 // batch_res :: 0.9808622002601624 \n",
            "batch_loss 17 :: 2.267530918121338 // batch_acc :: 0.2265625 // batch_res :: 0.9953206777572632 \n",
            "batch_loss 18 :: 2.266227960586548 // batch_acc :: 0.1484375 // batch_res :: 0.9836624264717102 \n",
            "batch_loss 19 :: 2.244859218597412 // batch_acc :: 0.2265625 // batch_res :: 0.9612771272659302 \n",
            "batch_loss 20 :: 2.2312207221984863 // batch_acc :: 0.1875 // batch_res :: 0.9606773257255554 \n",
            "batch_loss 21 :: 2.1970481872558594 // batch_acc :: 0.21875 // batch_res :: 0.972335934638977 \n",
            "batch_loss 22 :: 2.2173988819122314 // batch_acc :: 0.2109375 // batch_res :: 0.962627649307251 \n",
            "batch_loss 23 :: 2.1911158561706543 // batch_acc :: 0.2890625 // batch_res :: 0.968142032623291 \n",
            "batch_loss 24 :: 2.196098804473877 // batch_acc :: 0.234375 // batch_res :: 0.9721954464912415 \n",
            "batch_loss 25 :: 2.1563735008239746 // batch_acc :: 0.2109375 // batch_res :: 1.0212262868881226 \n",
            "batch_loss 26 :: 2.176654815673828 // batch_acc :: 0.2421875 // batch_res :: 0.9729909896850586 \n",
            "batch_loss 27 :: 2.164106845855713 // batch_acc :: 0.3359375 // batch_res :: 0.9623386263847351 \n",
            "batch_loss 28 :: 2.135467052459717 // batch_acc :: 0.1875 // batch_res :: 0.9871281385421753 \n",
            "batch_loss 29 :: 2.086947202682495 // batch_acc :: 0.2109375 // batch_res :: 1.0223422050476074 \n",
            "batch_loss 30 :: 2.063849449157715 // batch_acc :: 0.203125 // batch_res :: 0.9983127117156982 \n",
            "batch_loss 31 :: 2.1019558906555176 // batch_acc :: 0.1875 // batch_res :: 0.9924743175506592 \n",
            "batch_loss 32 :: 2.1666901111602783 // batch_acc :: 0.1484375 // batch_res :: 0.9682655930519104 \n",
            "batch_loss 33 :: 2.0071756839752197 // batch_acc :: 0.21875 // batch_res :: 1.0056025981903076 \n",
            "batch_loss 34 :: 2.067546844482422 // batch_acc :: 0.3359375 // batch_res :: 1.0060663223266602 \n",
            "batch_loss 35 :: 2.036883592605591 // batch_acc :: 0.2421875 // batch_res :: 0.9498036503791809 \n",
            "batch_loss 36 :: 1.936023235321045 // batch_acc :: 0.203125 // batch_res :: 0.9642392992973328 \n",
            "batch_loss 37 :: 1.9101847410202026 // batch_acc :: 0.2734375 // batch_res :: 0.9873716235160828 \n",
            "batch_loss 38 :: 2.021834135055542 // batch_acc :: 0.234375 // batch_res :: 0.9573309421539307 \n",
            "batch_loss 39 :: 1.9251703023910522 // batch_acc :: 0.21875 // batch_res :: 0.9562296867370605 \n",
            "batch_loss 40 :: 1.8321222066879272 // batch_acc :: 0.4140625 // batch_res :: 0.9987286329269409 \n",
            "batch_loss 41 :: 1.9372849464416504 // batch_acc :: 0.3203125 // batch_res :: 1.0064802169799805 \n",
            "batch_loss 42 :: 1.9748259782791138 // batch_acc :: 0.265625 // batch_res :: 0.9656569957733154 \n",
            "batch_loss 43 :: 1.9769991636276245 // batch_acc :: 0.484375 // batch_res :: 0.9819421768188477 \n",
            "batch_loss 44 :: 1.9016284942626953 // batch_acc :: 0.3828125 // batch_res :: 1.0134299993515015 \n",
            "batch_loss 45 :: 1.9019532203674316 // batch_acc :: 0.453125 // batch_res :: 1.0036821365356445 \n",
            "batch_loss 46 :: 1.8659875392913818 // batch_acc :: 0.46875 // batch_res :: 1.0123786926269531 \n",
            "batch_loss 47 :: 1.8938267230987549 // batch_acc :: 0.4296875 // batch_res :: 1.0155916213989258 \n",
            "batch_loss 48 :: 1.7660961151123047 // batch_acc :: 0.5234375 // batch_res :: 1.034185528755188 \n",
            "batch_loss 49 :: 1.8020721673965454 // batch_acc :: 0.4296875 // batch_res :: 1.0239202976226807 \n",
            "batch_loss 50 :: 1.8002090454101562 // batch_acc :: 0.484375 // batch_res :: 1.0303412675857544 \n",
            "batch_loss 51 :: 1.8825147151947021 // batch_acc :: 0.375 // batch_res :: 0.9999808073043823 \n",
            "batch_loss 52 :: 1.687943935394287 // batch_acc :: 0.5234375 // batch_res :: 1.0335019826889038 \n",
            "batch_loss 53 :: 1.8582475185394287 // batch_acc :: 0.421875 // batch_res :: 1.0787091255187988 \n",
            "batch_loss 54 :: 1.8085319995880127 // batch_acc :: 0.5 // batch_res :: 1.0213658809661865 \n",
            "batch_loss 55 :: 1.8904412984848022 // batch_acc :: 0.421875 // batch_res :: 1.0045578479766846 \n",
            "batch_loss 56 :: 1.5856305360794067 // batch_acc :: 0.671875 // batch_res :: 1.0544320344924927 \n",
            "batch_loss 57 :: 1.658179521560669 // batch_acc :: 0.421875 // batch_res :: 1.1068739891052246 \n",
            "batch_loss 58 :: 1.659069538116455 // batch_acc :: 0.4765625 // batch_res :: 1.0721410512924194 \n",
            "batch_loss 59 :: 1.6351243257522583 // batch_acc :: 0.6015625 // batch_res :: 1.0367534160614014 \n",
            "batch_loss 60 :: 1.612127423286438 // batch_acc :: 0.4765625 // batch_res :: 1.0411052703857422 \n",
            "batch_loss 61 :: 1.6664174795150757 // batch_acc :: 0.4609375 // batch_res :: 1.0624487400054932 \n",
            "batch_loss 62 :: 1.473929524421692 // batch_acc :: 0.5625 // batch_res :: 1.0571624040603638 \n",
            "batch_loss 63 :: 1.5442776679992676 // batch_acc :: 0.453125 // batch_res :: 1.0462251901626587 \n",
            "batch_loss 64 :: 1.3996467590332031 // batch_acc :: 0.546875 // batch_res :: 1.0794923305511475 \n",
            "batch_loss 65 :: 1.47278892993927 // batch_acc :: 0.46875 // batch_res :: 1.0861982107162476 \n",
            "batch_loss 66 :: 1.3965866565704346 // batch_acc :: 0.6328125 // batch_res :: 1.094612717628479 \n",
            "batch_loss 67 :: 1.270976185798645 // batch_acc :: 0.6953125 // batch_res :: 1.097338318824768 \n",
            "batch_loss 68 :: 1.1779303550720215 // batch_acc :: 0.6171875 // batch_res :: 1.109002947807312 \n",
            "batch_loss 69 :: 1.180550217628479 // batch_acc :: 0.578125 // batch_res :: 1.0880769491195679 \n",
            "batch_loss 70 :: 1.1820182800292969 // batch_acc :: 0.6640625 // batch_res :: 1.098615050315857 \n",
            "batch_loss 71 :: 1.2038476467132568 // batch_acc :: 0.6484375 // batch_res :: 1.0788257122039795 \n",
            "batch_loss 72 :: 1.1228951215744019 // batch_acc :: 0.6875 // batch_res :: 1.100396752357483 \n",
            "batch_loss 73 :: 1.144413709640503 // batch_acc :: 0.6796875 // batch_res :: 1.093239188194275 \n",
            "batch_loss 74 :: 1.2744132280349731 // batch_acc :: 0.6796875 // batch_res :: 1.056281328201294 \n",
            "batch_loss 75 :: 1.0573636293411255 // batch_acc :: 0.6015625 // batch_res :: 1.1002951860427856 \n",
            "batch_loss 76 :: 1.2545145750045776 // batch_acc :: 0.5625 // batch_res :: 1.1360771656036377 \n",
            "batch_loss 77 :: 1.005416989326477 // batch_acc :: 0.6796875 // batch_res :: 1.1126441955566406 \n",
            "batch_loss 78 :: 1.163942575454712 // batch_acc :: 0.6875 // batch_res :: 1.106328010559082 \n",
            "batch_loss 79 :: 0.9846089482307434 // batch_acc :: 0.625 // batch_res :: 1.1160863637924194 \n",
            "\tTRAIN epoch = 0 / loss = 1.8778496980667114 / acc = 0.3538000285625458 / res = 1.0369476079940796\n",
            "\tVAL epoch = 0 / loss = 4.81364631652832 / acc = 0.09400000423192978 / res = 1.3062169551849365\n",
            "batch_loss 1 :: 1.0727428197860718 // batch_acc :: 0.6640625 // batch_res :: 1.1474099159240723 \n",
            "batch_loss 2 :: 0.7921743988990784 // batch_acc :: 0.8125 // batch_res :: 1.1022186279296875 \n",
            "batch_loss 3 :: 1.023424744606018 // batch_acc :: 0.6484375 // batch_res :: 1.0702711343765259 \n",
            "batch_loss 4 :: 0.9567532539367676 // batch_acc :: 0.7578125 // batch_res :: 1.073678970336914 \n",
            "batch_loss 5 :: 0.7924163341522217 // batch_acc :: 0.7734375 // batch_res :: 1.0896577835083008 \n",
            "batch_loss 6 :: 1.0378389358520508 // batch_acc :: 0.625 // batch_res :: 1.106143593788147 \n",
            "batch_loss 7 :: 0.7389949560165405 // batch_acc :: 0.78125 // batch_res :: 1.0873725414276123 \n",
            "batch_loss 8 :: 1.0090208053588867 // batch_acc :: 0.65625 // batch_res :: 1.095862865447998 \n",
            "batch_loss 9 :: 0.9270445108413696 // batch_acc :: 0.75 // batch_res :: 1.0837632417678833 \n",
            "batch_loss 10 :: 0.8770501613616943 // batch_acc :: 0.734375 // batch_res :: 1.1028578281402588 \n",
            "batch_loss 11 :: 0.7195115685462952 // batch_acc :: 0.7890625 // batch_res :: 1.0970430374145508 \n",
            "batch_loss 12 :: 1.1076256036758423 // batch_acc :: 0.6328125 // batch_res :: 1.1008161306381226 \n",
            "batch_loss 13 :: 0.8057445287704468 // batch_acc :: 0.6953125 // batch_res :: 1.0929874181747437 \n",
            "batch_loss 14 :: 0.6141722202301025 // batch_acc :: 0.8359375 // batch_res :: 1.080701231956482 \n",
            "batch_loss 15 :: 0.6573538780212402 // batch_acc :: 0.8125 // batch_res :: 1.0773694515228271 \n",
            "batch_loss 16 :: 0.7409957647323608 // batch_acc :: 0.734375 // batch_res :: 1.0804780721664429 \n",
            "batch_loss 17 :: 0.7115381956100464 // batch_acc :: 0.7421875 // batch_res :: 1.0941674709320068 \n",
            "batch_loss 18 :: 0.6522983312606812 // batch_acc :: 0.8046875 // batch_res :: 1.0876096487045288 \n",
            "batch_loss 19 :: 0.6157779693603516 // batch_acc :: 0.796875 // batch_res :: 1.0915790796279907 \n",
            "batch_loss 20 :: 0.736945390701294 // batch_acc :: 0.78125 // batch_res :: 1.0906935930252075 \n",
            "batch_loss 21 :: 0.6797608137130737 // batch_acc :: 0.828125 // batch_res :: 1.0875180959701538 \n",
            "batch_loss 22 :: 0.7049815058708191 // batch_acc :: 0.8359375 // batch_res :: 1.0982985496520996 \n",
            "batch_loss 23 :: 0.6211422085762024 // batch_acc :: 0.8203125 // batch_res :: 1.1038002967834473 \n",
            "batch_loss 24 :: 0.6905639171600342 // batch_acc :: 0.78125 // batch_res :: 1.1237035989761353 \n",
            "batch_loss 25 :: 0.7305698394775391 // batch_acc :: 0.71875 // batch_res :: 1.111576795578003 \n",
            "batch_loss 26 :: 0.4626343250274658 // batch_acc :: 0.90625 // batch_res :: 1.107677698135376 \n",
            "batch_loss 27 :: 0.5308515429496765 // batch_acc :: 0.8125 // batch_res :: 1.0950146913528442 \n",
            "batch_loss 28 :: 0.6399734020233154 // batch_acc :: 0.796875 // batch_res :: 1.0781885385513306 \n",
            "batch_loss 29 :: 0.5567699074745178 // batch_acc :: 0.78125 // batch_res :: 1.0838052034378052 \n",
            "batch_loss 30 :: 0.5503318905830383 // batch_acc :: 0.8046875 // batch_res :: 1.0887268781661987 \n",
            "batch_loss 31 :: 0.5970025658607483 // batch_acc :: 0.78125 // batch_res :: 1.1155741214752197 \n",
            "batch_loss 32 :: 0.5109090805053711 // batch_acc :: 0.8828125 // batch_res :: 1.1345428228378296 \n",
            "batch_loss 33 :: 0.5983903408050537 // batch_acc :: 0.796875 // batch_res :: 1.1336891651153564 \n",
            "batch_loss 34 :: 0.48041173815727234 // batch_acc :: 0.8359375 // batch_res :: 1.1572612524032593 \n",
            "batch_loss 35 :: 0.44078636169433594 // batch_acc :: 0.890625 // batch_res :: 1.1433231830596924 \n",
            "batch_loss 36 :: 0.5143532752990723 // batch_acc :: 0.84375 // batch_res :: 1.1312588453292847 \n",
            "batch_loss 37 :: 0.4575802683830261 // batch_acc :: 0.8671875 // batch_res :: 1.1518162488937378 \n",
            "batch_loss 38 :: 0.38502827286720276 // batch_acc :: 0.8671875 // batch_res :: 1.1546679735183716 \n",
            "batch_loss 39 :: 0.5912554264068604 // batch_acc :: 0.8203125 // batch_res :: 1.1400911808013916 \n",
            "batch_loss 40 :: 0.5080404281616211 // batch_acc :: 0.796875 // batch_res :: 1.13710618019104 \n",
            "batch_loss 41 :: 0.5552060008049011 // batch_acc :: 0.859375 // batch_res :: 1.1302874088287354 \n",
            "batch_loss 42 :: 0.49220919609069824 // batch_acc :: 0.8515625 // batch_res :: 1.1168179512023926 \n",
            "batch_loss 43 :: 0.39657193422317505 // batch_acc :: 0.890625 // batch_res :: 1.1221697330474854 \n",
            "batch_loss 44 :: 0.46094048023223877 // batch_acc :: 0.8671875 // batch_res :: 1.121472716331482 \n",
            "batch_loss 45 :: 0.41982072591781616 // batch_acc :: 0.875 // batch_res :: 1.123751163482666 \n",
            "batch_loss 46 :: 0.3374722898006439 // batch_acc :: 0.8828125 // batch_res :: 1.151078224182129 \n",
            "batch_loss 47 :: 0.45018503069877625 // batch_acc :: 0.8828125 // batch_res :: 1.1659772396087646 \n",
            "batch_loss 48 :: 0.36163902282714844 // batch_acc :: 0.890625 // batch_res :: 1.1392734050750732 \n",
            "batch_loss 49 :: 0.786668062210083 // batch_acc :: 0.859375 // batch_res :: 1.1447139978408813 \n",
            "batch_loss 50 :: 0.492156445980072 // batch_acc :: 0.8515625 // batch_res :: 1.1838138103485107 \n",
            "batch_loss 51 :: 0.6124107837677002 // batch_acc :: 0.8515625 // batch_res :: 1.185254693031311 \n",
            "batch_loss 52 :: 0.4208562970161438 // batch_acc :: 0.828125 // batch_res :: 1.169199824333191 \n",
            "batch_loss 53 :: 0.40099307894706726 // batch_acc :: 0.8515625 // batch_res :: 1.1515908241271973 \n",
            "batch_loss 54 :: 0.3580039441585541 // batch_acc :: 0.875 // batch_res :: 1.1475977897644043 \n",
            "batch_loss 55 :: 0.5756920576095581 // batch_acc :: 0.8125 // batch_res :: 1.1397907733917236 \n",
            "batch_loss 56 :: 0.3924486041069031 // batch_acc :: 0.921875 // batch_res :: 1.1356850862503052 \n",
            "batch_loss 57 :: 0.3937133848667145 // batch_acc :: 0.890625 // batch_res :: 1.1361315250396729 \n",
            "batch_loss 58 :: 0.4797082245349884 // batch_acc :: 0.8203125 // batch_res :: 1.1330434083938599 \n",
            "batch_loss 59 :: 0.3097209930419922 // batch_acc :: 0.9453125 // batch_res :: 1.14457368850708 \n",
            "batch_loss 60 :: 0.4844345450401306 // batch_acc :: 0.8359375 // batch_res :: 1.1621726751327515 \n",
            "batch_loss 61 :: 0.4820256233215332 // batch_acc :: 0.8671875 // batch_res :: 1.1347484588623047 \n",
            "batch_loss 62 :: 0.3823249340057373 // batch_acc :: 0.875 // batch_res :: 1.1136716604232788 \n",
            "batch_loss 63 :: 0.38544130325317383 // batch_acc :: 0.9375 // batch_res :: 1.1170016527175903 \n",
            "batch_loss 64 :: 0.4104306101799011 // batch_acc :: 0.9296875 // batch_res :: 1.1265983581542969 \n",
            "batch_loss 65 :: 0.42356088757514954 // batch_acc :: 0.8984375 // batch_res :: 1.1382118463516235 \n",
            "batch_loss 66 :: 0.2592111825942993 // batch_acc :: 0.9375 // batch_res :: 1.1314506530761719 \n",
            "batch_loss 67 :: 0.4154379069805145 // batch_acc :: 0.875 // batch_res :: 1.1202524900436401 \n",
            "batch_loss 68 :: 0.3823321461677551 // batch_acc :: 0.90625 // batch_res :: 1.1400152444839478 \n",
            "batch_loss 69 :: 0.5292783975601196 // batch_acc :: 0.84375 // batch_res :: 1.1587715148925781 \n",
            "batch_loss 70 :: 0.3854394257068634 // batch_acc :: 0.859375 // batch_res :: 1.1567960977554321 \n",
            "batch_loss 71 :: 0.26685094833374023 // batch_acc :: 0.9296875 // batch_res :: 1.159183144569397 \n",
            "batch_loss 72 :: 0.38859498500823975 // batch_acc :: 0.8984375 // batch_res :: 1.1487842798233032 \n",
            "batch_loss 73 :: 0.2585229277610779 // batch_acc :: 0.921875 // batch_res :: 1.1454994678497314 \n",
            "batch_loss 74 :: 0.5310983657836914 // batch_acc :: 0.859375 // batch_res :: 1.1384751796722412 \n",
            "batch_loss 75 :: 0.37511754035949707 // batch_acc :: 0.8828125 // batch_res :: 1.1364214420318604 \n",
            "batch_loss 76 :: 0.21875952184200287 // batch_acc :: 0.9296875 // batch_res :: 1.1410483121871948 \n",
            "batch_loss 77 :: 0.2896142899990082 // batch_acc :: 0.9140625 // batch_res :: 1.1320810317993164 \n",
            "batch_loss 78 :: 0.3790315091609955 // batch_acc :: 0.890625 // batch_res :: 1.1437028646469116 \n",
            "batch_loss 79 :: 0.23043006658554077 // batch_acc :: 0.9375 // batch_res :: 1.1749378442764282 \n",
            "\tTRAIN epoch = 1 / loss = 0.5608127117156982 / acc = 0.8309000134468079 / res = 1.1233829259872437\n",
            "\tVAL epoch = 1 / loss = 9.832404136657715 / acc = 0.09400000423192978 / res = 1.2402125597000122\n",
            "batch_loss 1 :: 0.37043485045433044 // batch_acc :: 0.890625 // batch_res :: 1.1501408815383911 \n",
            "batch_loss 2 :: 0.3696223497390747 // batch_acc :: 0.890625 // batch_res :: 1.1429228782653809 \n",
            "batch_loss 3 :: 0.4914250373840332 // batch_acc :: 0.875 // batch_res :: 1.1405194997787476 \n",
            "batch_loss 4 :: 0.3363587260246277 // batch_acc :: 0.8828125 // batch_res :: 1.1407878398895264 \n",
            "batch_loss 5 :: 0.2824200391769409 // batch_acc :: 0.8984375 // batch_res :: 1.1362911462783813 \n",
            "batch_loss 6 :: 0.23826587200164795 // batch_acc :: 0.9140625 // batch_res :: 1.136947751045227 \n",
            "batch_loss 7 :: 0.46772900223731995 // batch_acc :: 0.8515625 // batch_res :: 1.130461573600769 \n",
            "batch_loss 8 :: 0.3386145532131195 // batch_acc :: 0.890625 // batch_res :: 1.1069401502609253 \n",
            "batch_loss 9 :: 0.3729249835014343 // batch_acc :: 0.875 // batch_res :: 1.0904209613800049 \n",
            "batch_loss 10 :: 0.20816826820373535 // batch_acc :: 0.9609375 // batch_res :: 1.1072207689285278 \n",
            "batch_loss 11 :: 0.39702537655830383 // batch_acc :: 0.890625 // batch_res :: 1.117889642715454 \n",
            "batch_loss 12 :: 0.2813846468925476 // batch_acc :: 0.9140625 // batch_res :: 1.1228053569793701 \n",
            "batch_loss 13 :: 0.2271491289138794 // batch_acc :: 0.8984375 // batch_res :: 1.1046993732452393 \n",
            "batch_loss 14 :: 0.41821354627609253 // batch_acc :: 0.828125 // batch_res :: 1.1015323400497437 \n",
            "batch_loss 15 :: 0.43788623809814453 // batch_acc :: 0.890625 // batch_res :: 1.1230982542037964 \n",
            "batch_loss 16 :: 0.6344746351242065 // batch_acc :: 0.7890625 // batch_res :: 1.1489344835281372 \n",
            "batch_loss 17 :: 0.18014594912528992 // batch_acc :: 0.9375 // batch_res :: 1.1565284729003906 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AvrarhHIP7qM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}